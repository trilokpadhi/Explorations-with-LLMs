{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpadhi1/miniconda3/envs/llama2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tpadhi1/miniconda3/envs/llama2/lib/python3.12/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 26.59it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,  445,  338,  263, 1243,    2,    2,    2,    2,    2],\n",
      "        [   1,  445,  338, 1790, 1243, 1206,  411,  263, 1422, 3309]])\n",
      "tensor([[ 1,  2,  3,  4,  5,  0,  0,  0,  0,  0],\n",
      "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n",
      "tensor([[15],\n",
      "        [55]])\n",
      "torch.Size([2, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://stackoverflow.com/questions/76926025/sentence-embeddings-from-llama-2-huggingface-opensource/77441536#77441536\n",
    "# Reference: https://stackoverflow.com/questions/63461262/bert-sentence-embeddings-from-transformers/64237402#64237402\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "t = AutoTokenizer.from_pretrained(model_id)\n",
    "t.pad_token = t.eos_token\n",
    "m = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\" )\n",
    "m.eval()\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"this is a test\",\n",
    "    \"this is another test case with a different length\",\n",
    "]\n",
    "t_input = t(texts, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_state = m(**t_input, output_hidden_states=True).hidden_states[-1]\n",
    "\n",
    "\n",
    "weights_for_non_padding = t_input.attention_mask * torch.arange(start=1, end=last_hidden_state.shape[1] + 1).unsqueeze(0)\n",
    "\n",
    "sum_embeddings = torch.sum(last_hidden_state * weights_for_non_padding.unsqueeze(-1), dim=1)\n",
    "num_of_none_padding_tokens = torch.sum(weights_for_non_padding, dim=-1).unsqueeze(-1)\n",
    "sentence_embeddings = sum_embeddings / num_of_none_padding_tokens\n",
    "\n",
    "print(t_input.input_ids)\n",
    "print(weights_for_non_padding)\n",
    "print(num_of_none_padding_tokens)\n",
    "print(sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted-Mean-Pooling\n",
    "Llama is a decoder with left-to-right attention. The idea behind weighted-mean_pooling is that the tokens at the end of the sentence should contribute more than the tokens at the beginning of the sentence because their weights are contextualized with the previous tokens, while the tokens at the beginning have far less context representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 8355.19it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token ids tensor([[   1,  445,  338,  263, 1243,    2,    2,    2,    2,    2],\n",
      "        [   1,  445,  338, 1790, 1243, 1206,  411,  263, 1422, 3309]])\n",
      "Weights tensor([[ 1,  2,  3,  4,  5,  0,  0,  0,  0,  0],\n",
      "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n",
      "tensor([[15],\n",
      "        [55]])\n",
      "torch.Size([2, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "t = AutoTokenizer.from_pretrained(model_id)\n",
    "t.pad_token = t.eos_token\n",
    "m = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\" )\n",
    "m.eval()\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"this is a test\",\n",
    "    \"this is another test case with a different length\",\n",
    "]\n",
    "t_input = t(texts, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_state = m(**t_input, output_hidden_states=True).hidden_states[-1]\n",
    "\n",
    "\n",
    "weights_for_non_padding = t_input.attention_mask * torch.arange(start=1, end=last_hidden_state.shape[1] + 1).unsqueeze(0)\n",
    "\n",
    "sum_embeddings = torch.sum(last_hidden_state * weights_for_non_padding.unsqueeze(-1), dim=1)\n",
    "num_of_none_padding_tokens = torch.sum(weights_for_non_padding, dim=-1).unsqueeze(-1)\n",
    "sentence_embeddings = sum_embeddings / num_of_none_padding_tokens\n",
    "\n",
    "print(\"Input token ids\",t_input.input_ids)\n",
    "print(\"Weights\", weights_for_non_padding)\n",
    "print(num_of_none_padding_tokens)\n",
    "print(sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt-based last token\n",
    "Another alternative is to use a certain prompt and utilize the contextualized embedding of the last token. This approach was introduced by: Jiang et al. and showed decent results for the OPT model family without finetuning. The idea is to force the model with a certain prompt to predict exactly one word. They call it PromptEOL and used the following implementation for their experiments:`\n",
    "\n",
    "```python\n",
    "\"This sentence: {text} means in one word:\"\n",
    "```\n",
    "\n",
    "Scaling sentence-based embeddings with LLMs : https://arxiv.org/abs/2307.16645 \n",
    "Please check their paper for further results. You can use the following code to utilize their approach with Llama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 9])\n",
      "torch.Size([2, 4096])\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# t = AutoTokenizer.from_pretrained(model_id)\n",
    "# t.pad_token = t.eos_token\n",
    "# m = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\" )\n",
    "# m.eval()\n",
    "\n",
    "\n",
    "# texts = [\n",
    "#     \"this is a test\",\n",
    "#     \"this is another test case with a different length\",\n",
    "# ]\n",
    "# prompt_template = \"This sentence: {text} means in one word:\"\n",
    "# texts = [prompt_template.format(text=x) for x in texts]\n",
    "\n",
    "# t_input = t(texts, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_state = m(**t_input, output_hidden_states=True, return_dict=True).hidden_states[-1]\n",
    "  \n",
    "idx_of_the_last_non_padding_token = t_input.attention_mask.bool().sum(1)-1\n",
    "sentence_embeddings = last_hidden_state[torch.arange(last_hidden_state.shape[0]), idx_of_the_last_non_padding_token]\n",
    "\n",
    "print(idx_of_the_last_non_padding_token)\n",
    "print(sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
